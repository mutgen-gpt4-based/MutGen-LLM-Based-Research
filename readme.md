# ðŸ§¬ Mutation Testing with GPT-4.1 for Java Code

This repository contains the final artifact of the research study exploring the use of **GPT-4.1** as the main component for **mutant generation in Java code**.

---

## ðŸ“‚ Project Structure Overview

### **1. Dataset â€” `dataset/raw_ast_diffs/`**

This directory contains all data related to the **main dataset** used to evaluate GPT-4.1 on two core tasks:

- Identification of mutant operators  
- Generation of mutants

Inside this folder, you will find **five clusters**, each stored in a separate subdirectory.  
Each cluster folder includes:

- Two text files: the **buggy** and **fixed** versions of the code  
- The **AST results** generated by GumTree  
- A **.csv file** containing the final dataset, filtered to include the most frequent mutant operators  

Each **fixedâ€“buggy pair** is tabulated with the mutant operators provided by GumTree.

---

### **2. Mutant Operator Filtering**

The filtering of mutant operators was necessary due to the **imbalance** in the number of Java nodes being modified.  
This imbalance is evident when comparing the **boxplot graphs** stored inside the `boxplot_freq/` directory of each cluster.

---

### **3. Step 1 â€” Identification of Mutant Operators**

Directory: `Step1/`

This step includes all information related to the **mutant operator identification phase**.  
There are **4 Attempts** in total, each containing:

- A **`prompts/`** folder with all prompts used to guide GPT-4.1  
- The **LangGraph** configuration files to orchestrate the LLM  
- A **`datasets_csv/`** folder containing the cleaned version of the main dataset  

#### ðŸ§  Execution

Each Attempt includes a Jupyter Notebook file:  
`async_run_attemptx.ipynb`  

To execute it:

1. Open the notebook in **Jupyter Lab**.  
2. Run all cells **sequentially**.  
3. The final section of the notebook will output the **mutant operator identification results**.

The final results for each Attempt are stored in their respective `results/` folders, containing `.csv` files with:

| Column | Description |
|--------|--------------|
| **fixed_code** | Fixed version of the commit |
| **buggy_code** | Buggy version before the commit |
| **gumtree_baseline** | Modifications proposed by GumTree |
| **line_in_dataset** | Line number in the main dataset where the commit is located |
| **gpt_attempt1** | Full response provided by GPT-4.1 |
| **baseline_nodes** | Nodes modified according to GumTree |
| **gpt_nodes** | Nodes identified by GPT-4.1 |
| **node_set_match** | Whether GPT-4.1 and GumTree identified the same nodes |
| **jaccard_score** | Similarity between GPT-4.1 and GumTree results |
| **precision, recall, f1_score** | Classification metrics |
| **TP, FP, FN** | True Positives, False Positives, and False Negatives |

---

### **4. Step 2 â€” Mutant Generation**

Directory: `Step2/`

This section includes all code related to the **mutant generation task**.

Inside `dataset_train/train_ast_cluster/`, you will find the **secondary dataset** â€” the same dataset used to train the **RNN models from Tufano et al.**

This secondary dataset was also filtered to maintain only **True Positives (TPs)** where the GumTree modifications occur on nodes that are part of the main dataset coverage.

The folder `Step2/dataset_csv/` contains the **filtered main dataset**, while the remaining folders define the **LangGraph configuration** for guiding the LLM in the mutant generation task.

#### âš™ï¸ Execution

Execution is similar to Step 1:

1. Open the notebook `main_graph_batch_gumtree_mutgen.ipynb`  
2. Run all cells sequentially  

The **final results** are located in `Step2/results/`, with the following information:

| Column | Description |
|--------|--------------|
| **fixed_code** | Code after the bug-fix commit |
| **buggy_code** | Code before the bug-fix commit |
| **line_in_dataset** | Line in the main dataset containing the specific TP |
| **mutant_version** | Mutant version generated by GPT-4.1 |
| **gumtree_baseline** | Mutant operators extracted from GumTree results |
| **exact_match** | Whether GPT-4.1's mutant is exactly the same as the buggy version |
| **similarity_gpt** | Semantic similarity between GPT-4.1 mutant and buggy code |
| **similarity_rnn** | Semantic similarity between RNN mutant and buggy code |
| **gpt_is_better** | Whether GPT-4.1 achieved higher similarity |
| **rnn_version** | Mutant version generated by the RNN model |
| **bleu_score_gpt** | Lexical similarity (BLEU) between GPT-4.1 mutant and buggy code |
| **bleu_score_rnn** | Lexical similarity (BLEU) between RNN mutant and buggy code |

---

## ðŸ§° Requirements

1. **Python â‰¥ 3.11**  
2. A `.env` file at the root of the project containing your OpenAI API key: 
>OPEN_API_KEY=your_api_key_here 
3. Install all dependencies listed in `requirements.txt`:  
```bash
pip install -r requirements.txt
```
> âš ï¸ **Note:**  
> The vectorized database for the **secondary dataset** is too large to be hosted in this repository.  
> If you wish to execute **Step 2**, you must generate the vector store manually.

### ðŸ§© How to Generate the Vector Store

Open the notebook located at:  
```Step2/module/crea_vector_store.ipynb```

